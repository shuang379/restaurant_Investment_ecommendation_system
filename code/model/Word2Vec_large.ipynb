{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Gensim uses Pythonâ€™s standard logging module to log various stuff at various priority levels; to activate logging (this is optional), run\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Amazon S3\n",
    "client = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Print out bucket names\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)\n",
    "bucket_name = 'cse6242oan-xchen668'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect psql server\n",
    "# psql --host cse6242project.cnsmcycpnqu7.us-east-1.rds.amazonaws.com --p --port 5432 --username=<your_name> --dbname=cse6242project\n",
    "engine = sqlalchemy.create_engine('postgresql+psycopg2://xchen668:password@cse6242project.cnsmcycpnqu7.us-east-1.rds.amazonaws.com/cse6242project')\n",
    "\n",
    "# business = pd.read_sql_query(\"SELECT * FROM {};\".format(\"business\"), engine)\n",
    "businessDf = pd.read_sql_table(\"business\", engine)\n",
    "\n",
    "# check data schema\n",
    "businessDf.head()\n",
    "# drop geom col for postGis\n",
    "businessDf.drop(\"geom\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select a.*, b.city\n",
    "    from review a\n",
    "    inner join \n",
    "    business b\n",
    "    on a.business_id = b.business_id\n",
    "    where b.is_us = 1\n",
    "    and b.is_restaurant = 1;\n",
    "\"\"\"\n",
    "usResReviews =  pd.read_sql_query(query, engine)\n",
    "\n",
    "print('\\nThe first review:\\n')\n",
    "print(usResReviews[\"text\"][0], '\\n')\n",
    "print(usResReviews.shape)\n",
    "print(usResReviews.columns)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "data = usResReviews[pd.notnull(usResReviews['text'])]\n",
    "print(data.shape)\n",
    "\n",
    "#size = 100000 #100,000\n",
    "size = 1000000\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "subdata, restdata = data.iloc[:size], data.iloc[size:]\n",
    "\n",
    "//subdata.to_csv('review_sub_{}.csv'.format(size), index=False, quoting=3, sep=',', escapechar='\\\\', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: long running time (150min)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# nltk.download('popular')\n",
    "# test tokenizer\n",
    "nltk.word_tokenize(\"Tokenize me\")\n",
    "\n",
    "from Word2VecUtility import Word2VecUtility\n",
    "\n",
    "t0 = time.time()\n",
    "review_sents = []\n",
    "# Cleaning and parsing the reviews...\n",
    "for i in range( 0, len(subdata[\"text\"])):\n",
    "    # sent_reviews += Word2VecUtility.review_to_sentences(data[\"text\"][i], tokenizer)\n",
    "    review_sents += Word2VecUtility.review_to_sentences(subdata.iloc[i][\"text\"])\n",
    "    \n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "\n",
    "with open('review_sents_{}.pkl'.format(size), 'wb') as out:\n",
    "    pickle.dump(review_sents, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[28]:\n",
    "\n",
    "\n",
    "model.wv.doesnt_match(\"man woman child kitchen\".split())\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "model.wv.doesnt_match(\"coffee tea juice restaurant\".split())\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "model.wv.most_similar(\"friendly\")\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "model.wv.most_similar(\"sushi\")\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "index2word_set = set(model.wv.index2word)\n",
    "print(len(index2word_set))\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "model.wv.most_similar(positive=['coffee'], topn=10)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "result = model.wv.most_similar(positive=['sushi'], negative=['japan'], topn=10)\n",
    "print(result)\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = model[model.wv.vocab]\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "list(model.wv.vocab.keys())\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "tsne_df = pd.DataFrame(X_tsne, columns=['x','y'])\n",
    "ax = sns.lmplot('x', 'y', tsne_df, fit_reg=False, size=8,\n",
    "               scatter_kws={'alpha':0.7,'s':60})\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "ax = None\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "labels = list(model.wv.vocab.keys())\n",
    "\n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(1000):\n",
    "    plt.scatter(X_tsne[i, 0], X_tsne[i, 1])\n",
    "    plt.annotate(labels[i],\n",
    "                 xy=(X_tsne[i, 0], X_tsne[i, 1]),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "labels = list(model.wv.vocab.keys())\n",
    "tsne_df[\"label\"] = labels\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "tsne_df[(tsne_df['x']>0) & (tsne_df['x']<3) & \n",
    "       (tsne_df['y']>35) & (tsne_df['y']<38)][:20]\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "filename = 'tsne_wordvects_{}.csv'.format(size)\n",
    "tsne_df.to_csv(filename, sep=',')\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "bucket_name = 'cse6242oan-xchen668'\n",
    "\n",
    "# Uploads the given file using a managed uploader, which will split up large\n",
    "# files automatically and upload parts in parallel.\n",
    "client.upload_file(filename, bucket_name, \"project/\" + filename)\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "print(restdata.shape)\n",
    "\n",
    "restdata.to_csv('review_rest.csv', index=False, quoting=3, sep=',', escapechar='\\\\', encoding='utf-8')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from Word2VecUtility import Word2VecUtility\n",
    "\n",
    "t0 = time.time()\n",
    "rest_review_sents = []\n",
    "# Cleaning and parsing the reviews...\n",
    "for i in range( 0, len(restdata[\"text\"])):\n",
    "    # sent_reviews += Word2VecUtility.review_to_sentences(data[\"text\"][i], tokenizer)\n",
    "    review_sents += Word2VecUtility.review_to_sentences(restdata.iloc[i][\"text\"])\n",
    "    \n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "with open('rest_review_sents.pkl', 'wb') as out:\n",
    "    pickle.dump(rest_review_sents, out)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "t0 = time.time()\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# continue to train the model (this will take some time)\n",
    "print(\"Training model...\")\n",
    "model.train(rest_review_sents, total_examples=len(rest_review_sents), epochs=model.epochs)\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context_{}\".format('all')\n",
    "model.save(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
